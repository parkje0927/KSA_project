### 1. 자연어처리 소개
#### 1.4 자연어처리 연구의 패러다임
- 규칙 기반, 통계 기반, 딥러닝 기반 

#### 1.5 딥러닝을 사용하는 자연어처리 연구
- 딥러닝을 사용하는 자연어처리의 연구 순서
- 단어 임베딩(word embedding)
	+ 자연어로 이루어진 문장을 컴퓨터가 입력 받을 수 있도록 하는 문장의 전처리 과정(모델의 일부)
	+ 다양한 방법이 존재하나, 단어간 연관성 등을 유지하는 벡터화 방법이 많이 쓰임
	+ 문법적으로만 사용되는 단어는 일반적으로 삭제
	+ 사전에 임베딩된 단어 사전을 사용하여 연구를 진행하는 경우 많음

- 코퍼스(Corpus), 모델
	+ 매우 많은 문장을 정제하여 모은 것
	+ Corpora(복수형)
	+ 통계 기반 & 딥러닝 기반 자연어처리에 가장 핵심인 자료
	+ 연구 필요성에 따라 문장 성분을 문장에 기입하거나 대응되는 번역문과 쌍을 구성하는 등, 연구에 사용할(모델이 학습해야 할) 정보를 함께 기입
	+ 예시) 형태소 분석 : I am a boy => <주어> <동사> <관사> <보어>

#### 1.6 딥러닝을 사용하는 다른 연구 분야와 자연어처리 비교
- AI를 활용한 주요 분야
	+ computer vision
	+ NLP
	+ speech processing

- 자연어처리
	+ 이산적인 값을 다룸(단어나 문장 단위)
	+ 분류 문제로 접근 가능
	+ 샘플의 확률 값을 구할 수 있음
	+ 문장 생성(자연어 생성) => auto-regressive 속성 지님(앞에 단어를 보고 뒤에 단어를 유추할 수 있음) / GAN 적용 불가

- 자연어처리의 필수 요건
	+ 언어적 지식 필요(예시-한국어의 언어적 특성은 무엇인가?)
	+ 어려운 전처리 과정(문제에 따른 정제 과정 필요)

---

### 2. 자연어처리를 위한 수학
#### 2.1 확률의 기초


---

### 4. 텍스트의 처리
#### 4.0 NLP 프로젝트 워크플로우
- 문제 정의(x, y정의) 
- -> 데이터 수집(문제 정의 고려한 수집, 레이블링) 
- -> 데이터 전처리 및 분석(형태 가공, 특수문자 및 의미 없는 문자 삭제) 
- -> 알고리즘 적용(가설 수립, 구현 및 적용) 
- -> 평가(실험 설계/수행, 테스트셋 구성) 
- -> 서비스 릴리즈(API 통합 배포, 필요시 유지 보수)
- 예시) 메일 -> 햄 or 스팸 / 채팅 -> 악성 or 악성이 아닌지 / sns 글 -> 긍정 or 부정

#### 4.0 NLP 전처리 워크플로우
- 코퍼스 수집(데이터)
	+ 구입 또는 외주 : 편하지만 품질이 보장되지 않는다.
	+ 직접 수집(크롤링) : 시간이 걸리지만 막상 하면 괜찮다.
- -> 다듬기(정제)
	+ 문제에 따른 노이즈 제거
	+ 인코딩 변환 : UTF-8 / UTF-16
- -> 라벨링
	+  문장 또는 단어 단위로
- -> 분절
	+ 형태소 분석기 활용
- -> 하위 단어 분할
	+ 단어보다 더 작은 의미 단위로 분할
- -> 배치화
	+ 사전 생성
	+ word2index 매핑
	+ 효율화를 위한 처리(성능에 영향을 주지 않는다면 길이가 비슷한 문장끼리 정렬된 상태로 학습시키는 것을 선호) 

#### 4.1 비정형 데이터 내의 오류
- 비정형 데이터
	+ 일정한 규격이나 형태를 지닌 숫자 데이터와 달리 그림이나 영상, 문서처럼 형태와 구조가 다른 구조화되지 않는 데이터
	+ 정의될 수 있는 확실한 기준이 없어 정의 내리고 이해하기 어려움
- 분석을 위해서는 비정형 데이터의 정형화가 요구됨. 

#### 4.2 텍스트 문서의 변환
- 목적으로 하는 파일로부터 텍스트를 추출하는 것이 전처리의 첫 번째 단계
	+ 일반적을 문서들은 사람이 읽기 간편한 형식으로 저장되어 있지만 파일 형식에 따라 저장 방법이 달라 시스템에게는 읽기 곤란
- '문서 파일'을 '문서'로 바꾸는 작업 수행 -> 목표 어휘언어의 문자만 남아있어야 함
- 코딩 관련 커맨드 제거, 필요하지 않은 특수문자 제거 -> 순수하게 문장 단위로 남은 텍스트
- 단어를 구분 짓는 잣대를 통일, 차후 텍스트 처리 시 일관성 부족으로 생길 오류나 불상사를 방지
- train, test, val 데이터에 대해 동일한 전처리를 해야 함
- 인터렉티브한 노이즈 제거 과정
	+ 규칙에 의해 노이즈를 제거하기 때문에 노이즈 전부를 제거하는 것은 어려움
	+ 따라서 반복적인 규칙생성 및 적용 과정이 필요
	+ 끝이 없는 과정 => 노력과 품질 사이의 trade-off

#### 4.3 띄어쓰기 교정 방법
- 띄어쓰기 : 한국어는 크게 의미분절, 가독성, 의미혼용 방지의 용도
- 형태소 분석기를 사용하는 규칙기반의 분석적인 방법 : koNLPy
- 어절 블록 양방향 알고리즘
- 통계, 확률 기반 : 수정방향이 옳을 확률이 높은 후보들을 차례로 나열
	
#### 4.4 철자 및 맞춤법 교정방법
##### 규칙기반
- 형태소 분석기로 하나의 어절이 입력된 어절을 형태소로 분절
- 분절 결과가 적합한지 접속 정보표를 이용하여 확인

##### 통계, 확률 기반
- Bayesian inference model
	+ 올바른 교정결과를 도출하기 위해 주어진 단어로부터 오타가 일어날 확률을 확률적으로 계산하는 방법
	+ 이미, p(a), p(b) 의 확률을 알고 있다. 
	+ 우리의 믿음을 첨가해주면, 데이터에 100% 의존하지 않아도 된다.
	+ 책 => 세상에서 가장 쉬운 베이즈통계학 입문

---

### 5. 어휘 분석(Lexial analysis)
#### 5.1 형태소 분석
##### 형태소 분석 절차
- 어휘 분석 : 단어의 구조를 식별하고 분석을 통한 어휘의 의미와 품사에 관한 단어 수준의 연구
- 형태소 분석 : 더 이상 분해될 수 없는 최소한의 의미 단위인 형태소를 자연어의 제약 조건과 문법 규칙에 맞춰 분석하는 것 
- 1) 단어에서 최소 의미를 포함하는 형태소로 분리한다.
	+ 형태소 분석의 처리 대상인 어절은 하나 이상의 형태소가 연결된 것
	+ 한국어에서 형태소가 연결될 때, 형태소의 변형이 일어나기 때문에 형태소 원형의 복원이 필요함
- 2) 형태론적 변형이 일어난 형태소의 원형을 찾는다.
	+ 형태소와 그 형태소의 품사를 쌍으로 나타낸 것을 형태소품사쌍이라고 함
- 3) 단어와 사전들 사이의 결합 조건에 따라 옳은 분석 후보를 선택한다. 
	+ 예시) "나_대명사" + "는_조사"

##### 영어 형태소 분석
- 영어에서 최소 단위의 의미를 갖는 기본 단위는 단어
	+ word : am / stemming : am / lemmatization(원형) : be
	+ word : has / stemming : ha / lemmatization(원형) : hava
- 일반적으로 영어의 형태소는 접사
- 접사를 제거했을 때 의미가 바뀌는 단어들이 존재하며, 최소한의 의미를 가진 형태소를 찾아 원형 분석 필요함

##### 한국어 형태소 분석 라이브러리
- 한국어 형태소 분석기의 오픈 라이브러리
	+ koNLPy : 한나눔, 코모란, 미캡, 꼬꼬마, 트위터
	+ Khiii : 딥러닝(CNN)을 이용한 형태소 분석기 

#### 5.2 품사 태깅
##### 품사 태깅이란?
- 품사 : 단어의 기능, 형태, 의미에 따라 나눈 것을 말함
- 태깅 : 같은 단어에 대해 의미가 다를 경우(중의성)를 해결하기 위해 부가적인 언어의 정보를 부착

##### 형태론적 중의성 해결 방법
- 자동 품사 태깅 방법
	+ 문맥틀 형식으로 규칙을 기술하는 방법
- 통계적 품사 태깅 방법
	+ 변형 마르코프 모형에 기반한 방법

##### 품사 태깅 접근법
- 규칙 기반의 접근법
	+ 언어 정보에서 생성되는 규칙 형태로 표현, 이를 적용하여 태깅을 수행함
	+ 긍정 정보, 부정 정보, 수정 정보를 이용하여 중의성을 해결하고 태깅을 부착하는 방법
	+ 뒤에 오는 단어를 바탕으로 추정한다.
	+ "나_동사" or "나_대명사" + "새" => 뒤에 "새"라는 단어를 통해 "나_동사" 라고 추정

- 통계 기반의 접근법(Hidden Markov Model)
	+ 태그가 부착된 대량의 코퍼스가 주어지면 태깅에 적합한 모델을 선정하고 코퍼스에서 추출된 통계정보를 이용
	+ 대표적으로 어휘 확률만을 이용하는 방법 : Hidden Markov Model(HMM) => 가장 성능이 좋은 접근 방법
	+ 태깅되지 않은 코퍼스로부터 unsupervised learning 을 통해 어휘 확률만을 획득
	+ 주어진 문장에서 형태소의 품사 태그 정보를 숨긴채로 확률 정보를 이용하여 가장 가능성이 높은 경로를 찾음
	
- 딥러닝 기반의 접근법
	+ 언어 처이에 있어서 딥러닝의 효과
	+ (1) 데이터로부터 특징을 자동으로 학습
	+ (2) 폭넓은 문제 정보를 다룰 수 있음
	+ (3) 모델에 적합한 출력을 다루기가 간단함
	+ (4) 언어가 아닌 이미지나 음성과 같은 모델들 간의 상호작용 가능, 멀티 모달 모델 구축 용이

---

### 6. 순환 신경망(RNN)
#### 6.1 기본 순환 신경망
##### RNN 소개
- 재귀적인 성향
- CNN과 다르게 hidden state 가 있다. 
- 이전 time-step의 출력값이 들어가게 된다.
- CNN : 표형태의 데이터, 영상이 입력으로 들어간다. 
- RNN : 연속 데이터, 시계열 데이터가 입력으로 들어간다. sequential data, time-series data

##### RNN 구조
- 1) Single-layered RNN
- input tensor
	+ time 이 기본 단위가 된다. 
	+ batch_size : 문장 / 1 : 단어 / input_size : 단어의 dimension
- output tensor
	+ batch_size :  / 1 :  / hidden_size : hidden-state size 
- 2) Multi-layered RNN(위로 쌓는다.)
- output tensor
	+ Single-layered RNN과 달리, 마지막 layer의 hidden state가 output이 된다. 
<br>

- 기본 RNN 에서 은닉 상태는 곧 출력
- Multi-layered RNN
	+ 출력은 마지막 레이어의 모든 시간 스텝의 은닉 상태
	+ 은닉 상태는 마지막 시간 스텝의 모든 레이어의 은닉 상태
- Bi-directional RNN
	+ 출력은 은닉 상태의 2배
	+ 은닉 상태는 레이어 개수의 2배
<br>

- 생성 task가 아니면, 한번에 X 입력, 한번에 Y 출력

##### RNN 활용 사례
- 다대일 : 입력은 many , 출력은 one
	+ 응용 사례 : 텍스트 분류
	+ 벡터가 1개인거지 들어가는 값이 1개인 것은 아니다. 
- 일대다 : 입력은 one, 출력은 mnay
	+ 응용 사례 : 자연어 생성, 기계번역
	+ 단어 하나가 입력이 들어가면 여러 개의 벡터가 나온다. 
	+ 특정 단어 하나만 주어질 때, 뒤에 문장을 채워보기, 소설 쓰기, 작곡 등이 해당된다. 
	+ seq2seq, encoder & decoder 구조이기도 하다. 
- 다대다 : 입력, 출력 many
	+ 응용 사례 : 문법 태깅, 형태소 분석
- 문장 전체가 다 들어가는 경우는 다대일, 다대다이고, 문장의 일부가 들어가는 경우는 일대다이다. 
<br>

- 두 가지 접근법
- 1) Non-autoregressive(non-generative)
	+ 현재 상태가 앞, 뒤 상태를 통해서 결정되는 경우
	+ 예) 품사 태깅(POS tagging, part of speech), 텍스트 분류(text classification)
	+ Bidirectional RNN 사용(권장) => 앞, 뒤 문장에서 다 영향을 미치기 때문이다. 
	+ 문장 전체를 읽어보고 task 수행(형태소, 품사, class 식별)
	+ sequence 전체를 입력으로 받는다. 
- 2) Autoregressive(generative)
	+ 현재 상태가 과거의 상태에 의존해서 결정되는 경우
	+ 예) 자연어 생성(NLG), 기계 번역(machine translation)
	+ 일대다(one-to-many) 경우에 해당
	+ Bidirectional RNN 사용 불가 => 모든 정보가 한 번에 주어지지 않기 때문이다.
	+ 미래를 모르는 경우
	+ 주로 sequence를 생성(챗봇, 기계 번역)

##### RNN의 학습(Back-propagatio through time(BPTT))
- 순방향 : 보통 입력값 x가 은닉층과 활성화 함수를 거쳐 h 와 출력값 y를 반환
- 역방향 : 출력부터 모든 시간 스템에 적용되는 gradient를 모두 계산
	+ RNN의 구조를 펼쳐보면(unfold), RNN을 시간 스텝 수만큼 히든 레이어를 가진 deep FNN으로 볼 수 있기 때문에 그와 동일한 방식으로 역전파를 수행(BPTT)
	+ backward propagation 이 time 축으로 간다고 보면된다.

#### 6.2 발전된 순환 신경망(Advanced RNN)
- 심층 신경망의 고질적인 문제 : gradient 소실과 폭주 문제 피하기, 고속 옵티마이저, 과대적합을 피하기 위한 규제 방법, 미리 훈련된 층 재사용 -> gradient vanishing(어떤 곳으로도 수렴되지 않고 학습이 되지 않는 문제), 느린 수렴(속도의 문제), overfitting => 전형적인 머신러닝 문제이다. 이것이 딥러닝으로도 넘어왔고 응용 분야에 따라서 시각, NLP 등등에서 공통적으로 나타나는 문제이다. 
	+ gradient vanishing 의 해결 방안 : He init(초기화를 잘 하기 위해), Activation(LeLU, LeakyLeLu / activation function 은 gradient vanishing 문제의 주요 원인이 된다.), Data normalization(BatchNormalization), Gradient Cilpping(RNN 사용하는 task 에서 널리 사용 됨 / gradient explosion 문제를 해결하기 위한 기법)
<br>

##### 그라디언트 소실 문제(gradient vanishing problem)
- RNN은 각 time-stamp 마다 tanh, sigmoid 등 활성화 함수를 통과함.
- 학습시 gradient 계산에서 이들을 미분하게 되면 0~1 사이 값을 반환하는 도함수
- RNN은 BPTT로 학습되므로 깊은 뉴럴 네트워크를 통과하게 됨. 이때 gradient에 0~1 사이값이 여러 번 곱해지면 gradient가 0에 수렴하게 됨. 학습이 되지 않는 문제
- 세타값이 업데이트가 안 되고, 학습이 안 되는 문제

##### Long short-term memory(LSTM)
- 그라디언트 소실 문제 해결 위해 제안됨
- time-step 사이에 은닉 상태와 더불어 셀 상태도 함께 전달
	+ cell state : 어떤 정보를 망각하고 기억할 지에 관한 판단이 반영된 정보 / 망각, 입력, 출력 게이트로 구성 
- 망각 게이트(Forget gate) : 현재 time-step t의 입력값 x_t와 이전 히든 스테이트 h_(t-1)을 고려해, <b>이전 셀 스테이트를 얼마나 보존할 지 판단 => 이런 역할을 아는 것이 중요하다.</b>
- 입력 게이트(Input gate) : 현재 time-step t의 입력값 x_t와 이전 히든 스테이트 h_(t-1)을 고려해, <b>셀 스테이트에 현재 상태에 대한 값을 얼마나 더할 지 판단</b>
	+ cell state와 hidden state 는 보통 같은 차원이다. 
	+ 256차원이라고 가정할 때, hidden state 는 256차원이다. 
	+ time 축에 대한 gradient vanishing 은 해결 -> sequence 길이는 길어짐 but layer 더 쌓는 건 불가능(최대 4개 쌓으면 잘 돌아간다.)
- 입력 게이트와 망각 게이트로부터 얻은 값을 통해 셀 스테이트 업데이트
- 출력 게이트(Output gate) : 업데이트된 셀 스테이트와 x_t와 h_(t-1)을 고려해 히든 스테이트를 업데이트하고, 다음 time-step t+1로 전달

##### Gated recurrent unit(GRU)
- 게이트 순환 유닛
- LSTM 보다 간소화되었으며, 별도의 셀 스테이트 없이 두 개의 게이트를 사용하여 그라디언트 소실 문제를 해결하고자 함
- 리셋 게이트와 업데이트 게이트로 구성됨
- 리셋 게이트(Reset gate) : 이전 히든 스테이트 h_(t-1)과 현재 입력 값 x_t를 고려해, 현재 입력 값을 히든 스테이트 h_t에 얼마나 반영할 지 판단
- 업데이트 게이트(Update gate) : 히든 스테이트 h_(t-1)과 입력값 x_t로부터 z_t값을 생성하고, z_t를 기준으로 리셋 게이트로부터 반환된 값과 이전 히든 스테이트 중 어디에 얼만큼 가중치를 둘 지 판단
<br>

- LSTM은 기본 RNN에 비해 훨씬 많은 파라미터 가짐
- LSTM이 그라디언트 소실 문제를 해소했지만, 무작정 긴 데이터를 모두 기억할 수 있는 건 아님
- 요즘에는 attention을 통해 이를 해결(100 time-step 을 허용)

##### 그라디언트 클리핑(gradient clipping)
- 그라디언트 폭주 문제에서 사용되는 기법
- 역전파 시 임계값을 넘지 못하게 그라디언트를 단순히 자르자
- 순환 신경망에서 널리 사용됨(RNN)
- time-step이 길어질수록 BPTT 알고리즘에서 gradient의 덧셈이 많아짐
	+ 긴 sequence인 경우 더 심화되는 증상
- gradient 소실 : activation function에서 양 끝 지점의 grad 값 나오는 경우
- gradient 폭주 : activation function에서 가운데에 grad 값 나오는 경우
- Adam optimizer 를 사용할 경우 크게 쓰이지 않음(이전에는 threshold, learning rate을 조절해야 했음)















	
	